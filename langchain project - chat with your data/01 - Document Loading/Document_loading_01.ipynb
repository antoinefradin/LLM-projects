{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c3ab93",
   "metadata": {},
   "source": [
    "# Document Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb75032-b899-48c5-8409-bfacbe92569c",
   "metadata": {},
   "source": [
    "The steps in this notebook include: \n",
    "- **Use Langchain Document Loaders** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c4e72-dca4-499d-8f07-ce297eb185aa",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Installation](#installation)\n",
    "2. [PDFs](#pdf)\n",
    "3. [Youtube](#youtube)  \n",
    "4. [Notion](#notion)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b4cbc-51ec-4d51-b6e9-d0f03411b356",
   "metadata": {},
   "source": [
    "**Source:** https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/2/document-loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b8ee8",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation\n",
    " \n",
    "In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. \n",
    "\n",
    "This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334357c",
   "metadata": {},
   "source": [
    "![overview.png](./images/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdedf25-2199-428a-b3c7-2d6175322837",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878cf09-4c18-4f18-8076-ecbada832155",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Installation** <a name=\"installation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f015f708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.0.332)\n",
      "Requirement already satisfied: openai in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: anyio<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (0.0.62)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (1.22.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.5.7)\n",
      "Requirement already satisfied: httpcore in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: pytest-subtests<0.12.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langsmith<0.1.0,>=0.0.52->langchain) (0.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: pytest>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain) (7.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (3.0.9)\n",
      "Requirement already satisfied: iniconfig in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pytest>=7.0->pytest-subtests<0.12.0,>=0.11.0->langsmith<0.1.0,>=0.0.52->langchain) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae425a-276a-491e-aede-24d856984926",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171310e-c627-47b7-a894-3f97320c5121",
   "metadata": {},
   "source": [
    "**Load OpenAI api key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26c2a2a-f7a7-4580-ab73-178bab918dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "# Load from a .env file \n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJhcHAiLCJzdWIiOiIxNDYyNzU5IiwiYXVkIjoiV0VCIiwiaWF0IjoxNjk5NDUxNzMzLCJleHAiOjE3MDAwNTY1MzN9.7mqcOZ3w4gd7m9QGWcdOx7U1ayk1l22LNZ8LfPOLqjE\"\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739082e1-4967-4335-a2f9-5d4af2f25dc8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38baf6d3",
   "metadata": {},
   "source": [
    "# **PDFs** <a name=\"pdf\"></a>\n",
    "\n",
    "We load a PDF [transcript](https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf) (These documents are the result of automated transcription so words and sentences are sometimes split unexpectedly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4583ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pypdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2339c18-d966-4e53-9c59-f9212a028316",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ef5d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284cc8a",
   "metadata": {},
   "source": [
    "Each page is a `Document`.  \n",
    "- A `Document` contains text (`page_content`) and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd28c723-3625-4219-b0f8-8d5b761ae79e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26ff4112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \\nof the class, and then we\\'ll start to  talk a bit about machine learning.  \\nBy way of introduction, my name\\'s  Andrew Ng and I\\'ll be instru ctor for this class. And so \\nI personally work in machine learning, and I\\' ve worked on it for about 15 years now, and \\nI actually think that machine learning is th e most exciting field of all the computer \\nsciences. So I\\'m actually always excited about  teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thin g in computer science, but \\nthe most exciting thing in all of human e ndeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learni ng and all aspects of machin e learning. Paul Baumstarck \\nworks in machine learning and computer vision.  Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computa tional biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is  the head TA — he\\'s head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he\\'s not here  — Daniel applies l earning algorithms to \\nproblems in natural language processing.  \\nSo you\\'ll get to know the TAs and me much be tter throughout this quarter, but just from \\nthe sorts of things the TA\\'s do, I hope you can  already tell that machine learning is a \\nhighly interdisciplinary topic in which just the TAs find l earning algorithms to problems \\nin computer vision and biology and robots a nd language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in  computer systems or databases to economists \\nand sort of also an unending stream of  people from industry coming to Stanford \\ninterested in applying machine learni ng methods to their own problems.  \\nSo yeah, this is fun. A couple of weeks ago, a student actually forwar ded to me an article \\nin \"Computer World\" about the 12 IT skills th at employers can\\'t say no to. So it\\'s about \\nsort of the 12 most desirabl e skills in all of IT and all of information technology, and \\ntopping the list was actually machine lear ning. So I think this is a good time to be \\nlearning this stuff and learning algorithms and having a large impact on many segments \\nof science and industry.  \\nI\\'m actually curious about something. Learni ng algorithms is one of the things that \\ntouches many areas of science and industrie s, and I\\'m just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from  EE? Oh, okay, maybe about a fifth. How ', metadata={'source': 'data/MachineLearning-Lecture01.pdf', 'page': 0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = pages[0]\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c94e3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is ju st spend a little time going over the logistics \n",
      "of the class, and then we'll start to  talk a bit about machine learning.  \n",
      "By way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \n",
      "I personally work in machine learning, and I' ve worked on it for about 15 years now, and \n",
      "I actually think that machine learning i\n"
     ]
    }
   ],
   "source": [
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605d0932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/MachineLearning-Lecture01.pdf', 'page': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff1db2-2a71-4dcd-9f7b-d52f046d6041",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead28868",
   "metadata": {},
   "source": [
    "# **YouTube** <a name=\"youtube\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5f360f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4835edd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt_dlp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2023.10.13)\n",
      "Requirement already satisfied: mutagen in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yt_dlp) (1.47.0)\n",
      "Requirement already satisfied: pycryptodomex in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yt_dlp) (3.19.0)\n",
      "Requirement already satisfied: websockets in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yt_dlp) (12.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yt_dlp) (2023.5.7)\n",
      "Requirement already satisfied: brotli in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yt_dlp) (1.1.0)\n",
      "Requirement already satisfied: youtube-dl in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2021.12.17)\n",
      "Requirement already satisfied: pydub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: ffmpeg in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.4)\n",
      "Requirement already satisfied: ffprobe in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.5)\n",
      "Requirement already satisfied: ffmpeg-python in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ffmpeg-python) (0.18.3)\n",
      "/bin/sh: apt-get: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install -U yt_dlp\n",
    "!pip install -U youtube-dl\n",
    "!pip install pydub\n",
    "!pip install ffmpeg \n",
    "!pip install ffprobe\n",
    "!pip install ffmpeg-python\n",
    "!apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0ece1-8db3-463e-b516-c09086993f9b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa8f7a-bd04-4bbd-96a9-8c2088426885",
   "metadata": {},
   "source": [
    "**Note**: This can take several minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f559468-7199-4f83-b623-7f80b69f4294",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9663e3e-2def-43ea-ab8d-42267a4ee9c1",
   "metadata": {},
   "source": [
    "- **Generic Document Loader:** A generic document loader that allows combining an arbitrary blob loader with a blob parser.  \n",
    "    Parameters :    \n",
    "        `blob_loader` – A blob loader which knows how to yield blobs  \n",
    "        `blob_parser` – A blob parser which knows how to parse blobs into documents\n",
    "\n",
    "- **OpenAIWhisperParser():** Transcribe and parse audio files. Audio transcription is with OpenAI Whisper model. (Whisper = open source automatic speech recognition model (ASR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197f0936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n",
    "save_dir=\"./\"\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url],save_dir),\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "#docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bf39c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06160df-3cfb-4c83-ad1a-7774ec0e85a6",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54e6f9",
   "metadata": {},
   "source": [
    "# **URLs** <a name=\"url\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ede7f5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://openai.com/blog/new-models-and-developer-products-announced-at-devday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596c8f4e-6fd5-4230-9dfc-84e100e90d72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\nNew models and developer products announced at DevDay\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit BlogNew models and developer products announced at DevDayGPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4 Turbo with Vision, DALL·E 3 API, and more.November 6, 2023AuthorsOpenAI Announcements,\\xa0ProductToday, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context windowNew Assistants API that makes it easier for developers to build their own assistive AI apps that have goals and can call models and toolsNew multimodal capabilities in the platform, including vision, image creation (DALL·E 3), and text-to-speech (TTS)We’ll begin rolling out new features to OpenAI customers starting at 1pm PT today.Learn more about OpenAI DevDay announcements for ChatGPT.GPT-4 Turbo with 128K contextWe released the first version of GPT-4 in March and made GPT-4 generally available to all developers in July. Today we’re launching a preview of the next generation of this model, GPT-4 Turbo.\\xa0GPT-4 Turbo is more capable and has knowledge of world events up to April 2023. It has a 128k context window so it can fit the equivalent of more than 300 pages of text in a single prompt. We also optimized its performance so we are able to offer GPT-4 Turbo at a 3x cheaper price for input tokens and a 2x cheaper price for output tokens compared to GPT-4.GPT-4 Turbo is available for all paying developers to try by passing gpt-4-1106-preview in the API and we plan to release the stable production-ready model in the coming weeks.Function calling updatesFunction calling lets you describe functions of your app or external APIs to models, and have the model intelligently choose to output a JSON object containing arguments to call those functions. We’re releasing several improvements today, including the ability to call multiple functions in a single message: users can send one message requesting multiple actions, such as “open the car window and turn off the A/C”,\\xa0which would previously require multiple roundtrips with the model (learn more). We are also improving function calling accuracy: GPT-4 Turbo is more likely to return the right function parameters.Improved instruction following and JSON modeGPT-4 Turbo performs better than our previous models on tasks that require the careful following of instructions, such as generating specific formats (e.g., “always respond in XML”). It also supports our new JSON mode, which ensures the model will respond with valid JSON. The new API parameter response_format enables the model to constrain its output to generate a syntactically correct JSON object. JSON mode is useful for developers generating JSON in the Chat Completions API outside of function calling.Reproducible outputs and log probabilitiesThe new seed parameter enables reproducible outputs by making the model return consistent completions most of the time. This beta feature is useful for use cases such as replaying requests for debugging, writing more comprehensive unit tests, and generally having a higher degree of control over the model behavior. We at OpenAI have been using this feature internally for our own unit tests and have found it invaluable. We’re excited to see how developers will use it. Learn more.We’re also launching a feature to return the log probabilities for the most likely output tokens generated by GPT-4 Turbo and GPT-3.5 Turbo in the next few weeks, which will be useful for building features such as autocomplete in a search experience.Updated GPT-3.5 TurboIn addition to GPT-4 Turbo, we are also releasing a new version of GPT-3.5 Turbo that supports a 16K context window by default. The new 3.5 Turbo supports improved instruction following, JSON mode, and parallel function calling. For instance, our internal evals show a 38% improvement on format following tasks such as generating JSON, XML and YAML. Developers can access this new model by calling gpt-3.5-turbo-1106 in the API. Applications using the gpt-3.5-turbo name will automatically be upgraded to the new model on December 11. Older models will continue to be accessible by passing gpt-3.5-turbo-0613 in the API until June 13, 2024. Learn more.Assistants API, Retrieval, and Code InterpreterToday, we’re releasing the Assistants API, our first step towards helping developers build agent-like experiences within their own applications. An assistant is a purpose-built AI that has specific instructions, leverages extra knowledge, and can call models and tools to perform tasks. The new Assistants API provides new capabilities such as Code Interpreter and Retrieval as well as function calling to handle a lot of the heavy lifting that you previously had to do yourself and enable you to build high-quality AI apps.This API is designed for flexibility; use cases range from a natural language-based data analysis app, a coding assistant, an AI-powered vacation planner, a voice-controlled DJ, a smart visual canvas—the list goes on. The Assistants API is built on the same capabilities that enable our new GPTs product: custom instructions and tools such as Code interpreter, Retrieval, and function calling.A key change introduced by this API is persistent and infinitely long threads, which allow developers to hand off thread state management to OpenAI and work around context window constraints. With the Assistants API, you simply add each new message to an existing thread.Assistants also have access to call new tools as needed, including:Code Interpreter: writes and runs Python code in a sandboxed execution environment, and can generate graphs and charts, and process files with diverse data and formatting. It allows your assistants to run code iteratively to solve challenging code and math problems, and more.Retrieval: augments the assistant with knowledge from outside our models, such as proprietary domain data, product information or documents provided by your users. This means you don’t need to compute and store embeddings for your documents, or implement chunking and search algorithms. The Assistants API optimizes what retrieval technique to use based on our experience building knowledge retrieval in ChatGPT.Function calling: enables assistants to invoke functions you define and incorporate the function response in their messages.As with the rest of the platform, data and files passed to the OpenAI API are never used to train our models and developers can delete the data when they see fit.You can try the Assistants API beta without writing any code by heading to the Assistants playground.Use the Assistants playground to create high quality assistants without code.The Assistants API is in beta and available to all developers starting today. Please share what you build with us (@OpenAI) along with your feedback which we will incorporate as we continue building over the coming weeks. Pricing for the Assistants APIs and its tools is available on our pricing page.New modalities in the APIGPT-4 Turbo with visionGPT-4 Turbo can accept images as inputs in the Chat Completions API, enabling use cases such as generating captions, analyzing real world images in detail, and reading documents with figures. For example, BeMyEyes uses this technology to help people who are blind or have low vision with daily tasks like identifying a product or navigating a store. Developers can access this feature by using gpt-4-vision-preview in the API. We plan to roll out vision support to the main GPT-4 Turbo model as part of its stable release. Pricing depends on the input image size. For instance, passing an image with 1080×1080 pixels to GPT-4 Turbo costs $0.00765. Check out our vision guide.DALL·E 3Developers can integrate DALL·E 3, which we recently launched to ChatGPT Plus and Enterprise users, directly into their apps and products through our Images API by specifying dall-e-3 as the model. Companies like Snap, Coca-Cola, and Shutterstock have used DALL·E 3 to programmatically generate images and designs for their customers and campaigns. Similar to the previous version of DALL·E, the API incorporates built-in moderation to help developers protect their applications against misuse. We offer different format and quality options, with prices starting at $0.04 per image generated. Check out our guide to getting started with DALL·E 3 in the API.Text-to-speech (TTS)Developers can now generate human-quality speech from text via the text-to-speech API. Our new TTS model offers six preset voices to choose from and two model variants, tts-1 and tts-1-hd. tts is optimized for real-time use cases and tts-1-hd is optimized for quality. Pricing starts at $0.015 per input 1,000 characters. Check out our TTS guide to get started.Listen to voice samplesSelect textScenicDirectionsTechnicalRecipeAs the golden sun dips below the horizon, casting long shadows across the tranquil meadow, the world seems to hush, and a sense of calmness envelops the Earth, promising a peaceful night’s rest for all living beings.Select voiceAlloyEchoFableOnyxNovaShimmerModel customizationGPT-4 fine tuning experimental accessWe’re creating an experimental access program for GPT-4 fine-tuning. Preliminary results indicate that GPT-4 fine-tuning requires more work to achieve meaningful improvements over the base model compared to the substantial gains realized with GPT-3.5 fine-tuning. As quality and safety for GPT-4 fine-tuning improves, developers actively using GPT-3.5 fine-tuning will be presented with an option to apply to the GPT-4 program within their fine-tuning console.Custom modelsFor organizations that need even more customization than fine-tuning can provide (particularly applicable to domains with extremely large proprietary datasets—billions of tokens at minimum), we’re also launching a Custom Models program, giving selected organizations an opportunity to work with a dedicated group of OpenAI researchers to train custom GPT-4 to their specific domain. This includes modifying every step of the model training process, from doing additional domain specific pre-training, to running a custom RL post-training process tailored for the specific domain. Organizations will have exclusive access to their custom models. In keeping with our existing enterprise privacy policies, custom models will not be served to or shared with other customers or used to train other models.\\xa0Also, proprietary data provided to OpenAI to train custom models will not be reused in any other context. This will be a very limited (and expensive) program to start—interested orgs can apply here.Lower prices and higher rate limitsLower pricesWe’re decreasing several prices across the platform to pass on savings to developers (all prices below are expressed per 1,000 tokens):GPT-4 Turbo input tokens are 3x cheaper than GPT-4 at $0.01 and output tokens are 2x cheaper at $0.03.GPT-3.5 Turbo input tokens are 3x cheaper than the previous 16K model at $0.001 and output tokens are 2x cheaper at $0.002. Developers previously using GPT-3.5 Turbo 4K benefit from a 33% reduction on input tokens at $0.001. Those lower prices only apply to the new GPT-3.5 Turbo introduced today.Fine-tuned GPT-3.5 Turbo 4K model input tokens are reduced by 4x at $0.003 and output tokens are 2.7x cheaper at $0.006. Fine-tuning also supports 16K context at the same price as 4K with the new GPT-3.5 Turbo model. These new prices also apply to fine-tuned gpt-3.5-turbo-0613 models.Older modelsNew modelsGPT-4 TurboGPT-4 8K\\nInput: $0.03\\nOutput: $0.06\\n\\nGPT-4 32K\\nInput: $0.06\\nOutput: $0.12GPT-4 Turbo 128K\\nInput: $0.01\\nOutput: $0.03GPT-3.5 TurboGPT-3.5 Turbo 4K\\nInput: $0.0015\\nOutput: $0.002\\n\\nGPT-3.5 Turbo 16K\\nInput: $0.003\\nOutput: $0.004GPT-3.5 Turbo 16K\\nInput: $0.001\\nOutput: $0.002GPT-3.5 Turbo fine-tuningGPT-3.5 Turbo 4K fine-tuning\\nTraining: $0.008\\nInput: $0.012\\nOutput: $0.016GPT-3.5 Turbo 4K and 16K fine-tuning\\nTraining: $0.008\\nInput: $0.003\\nOutput: $0.006Higher rate limitsTo help you scale your applications, we’re doubling the tokens per minute limit for all our paying GPT-4 customers. You can view your new rate limits in your rate limit page. We’ve also published our usage tiers that determine automatic rate limits increases, so you know what to expect in how your usage limits will automatically scale. You can now request increases to usage limits from your account settings.Copyright ShieldOpenAI is committed to protecting our customers with built-in copyright safeguards in our systems. Today, we’re going one step further and introducing Copyright Shield—we will now step in and defend our customers, and pay the costs incurred, if you face legal claims around copyright infringement. This applies to generally available features of ChatGPT Enterprise and our developer platform.Whisper v3 and Consistency DecoderWe are releasing Whisper large-v3, the next version of our open source automatic speech recognition model (ASR) which features improved performance across languages. We also plan to support Whisper v3 in our API in the near future.We are also open sourcing the Consistency Decoder, a drop in replacement for the Stable Diffusion VAE decoder. This decoder improves all images compatible with the by Stable Diffusion 1.0+ VAE, with significant improvements in text, faces and straight lines.Learn more about our OpenAI DevDay announcements for ChatGPT.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALL·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI © 2015\\u200a–\\u200a2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top\\n', metadata={'source': 'https://openai.com/blog/new-models-and-developer-products-announced-at-devday', 'title': 'New models and developer products announced at DevDay', 'description': 'GPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4 Turbo with Vision, DALL·E 3 API, and more.', 'language': 'en-US'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3039f8ed-ebc1-44e7-829a-9499dc5d1f03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "New models and developer products announced at DevDay\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALL·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchOverviewIndexGPT-4DALL·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer stories Quick Links Log inTry ChatGPTSearch Submit BlogNew models and developer products announced at DevDayGPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4 Turbo with Vision, DALL·E 3 API, and more.November 6, 2023AuthorsOpenAI Announcements, ProductToday, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context windowNew Assistants API that makes it easier for developers to build their own assistive AI apps that have goals and can call models and toolsNew multimodal capabilities in the platform, including vision, image creation (DALL·E 3), and text-to-speech (TTS)We’ll begin rolling out new features to OpenAI customers starting at 1pm PT today.Learn more about OpenAI DevDay announcements for ChatGPT.GPT-4 Turbo with 128K contextWe released the first version of G\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e54ecc-d221-49ee-8968-12afae758826",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c62299",
   "metadata": {},
   "source": [
    "# **Notion** <a name=\"notion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4b630",
   "metadata": {},
   "source": [
    "Follow steps [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion) for an example Notion site such as [this one](https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f):\n",
    "\n",
    "* Duplicate the page into your own Notion space and export as `Markdown / CSV`.\n",
    "* Unzip it and save it as a folder that contains the markdown file for the Notion page.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17ebad",
   "metadata": {},
   "source": [
    "<img src=\"images/image_notion.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca83ff-9918-47ce-ba2a-83fee19e3ba9",
   "metadata": {},
   "source": [
    "- When exporting, make sure to select the _Markdown & CSV format_ option.\n",
    "\n",
    "This will produce a `.zip` file. Move the .zip file into this repository and run the following command to unzip the zip file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f440465-a64f-4e95-9963-80e486f5aee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/27261042-ae4b-4a74-b2d7-6154d7246eb4_Export-0d6d611c-c647-4296-b0f6-4c989f8c5d0d.zip\n",
      "  inflating: data/Notion_DB/MLOps tools 2023 830865f5e014447eb4c8c2cf5dbb7367.md  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o \"data/27261042-ae4b-4a74-b2d7-6154d7246eb4_Export-0d6d611c-c647-4296-b0f6-4c989f8c5d0d.zip\" -d \"data/Notion_DB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13103b6-93bb-412c-961a-0f26405ffed0",
   "metadata": {},
   "source": [
    "> **`unzip` Command Options**:  \n",
    ">The unzip command has various options:\n",
    ">\n",
    ">`-o` – overwrite existing files without prompting.   \n",
    "> `-d exdir` - An optional directory to which to extract files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896622c-a309-4429-8094-593ee347cc8f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292e2a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"data/Notion_DB\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ce8fde2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MLOps tools 2023\n",
      "\n",
      "[https://i0.wp.com/neptune.ai/wp-content/uploads/2023/07/MLOps-Landscape-in-2023-Top-Tools-and-Platforms-5.png?ssl=1](https://i0.wp.com/neptune.ai/wp-content/uploads/2023/07/MLOps-\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f14fa1d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/Notion_DB/MLOps tools 2023 830865f5e014447eb4c8c2cf5dbb7367.md'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b1717-7532-415a-9094-18da766cefc0",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
